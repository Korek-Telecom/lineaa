{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first in a series of posts in which I'll be exploring machine learning concepts. The series is loosely based on the sequence from Andrew Ng's [Introduction to Machine Learning](https://www.coursera.org/learn/machine-learning) course on Coursera, but won't cover every part of the coursework, and may veer into interesting tangents along the way. \n",
    "\n",
    "In each post that I cover a major machine learning algorithm, I'll first implement it from scratch, using standard Python data science tools like numpy, scikit-learn and others. Then, I'll build it in [Tensorflow](https://www.tensorflow.org/), a powerful new machine learning library from Google. \n",
    "\n",
    "The first algorithm is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). Linear regression attempts to fit a line of best fit to a data set, using one or more features as coefficients for a linear equation. This algorithm is the most basic in machine learning and should look familiar to anyone who has been exposed to even a little bit of statistics. \n",
    "\n",
    "# Generating and Plotting Data \n",
    "\n",
    "I'm using the [`make_regression`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) function from [scikit-learn](http://scikit-learn.org/stable/index.html) to generate a simple data set that will illustrate this algorithm well. This function takes a number of parameters to tune its output, but here the important ones are the number of samples (100), the number of features (1, allowing a 2D plot of the data and regression), and noise, which adds a little random variation so the points don't lie *exactly* on a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (100, 1)\n",
      "y: (100,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=15.0)\n",
    "\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns two outputs, each a one-dimensional array. The first, XX, is a 100 row, 1 column matrix, representing 100 samples with 1 feature each. The second, yy, is a 100 element vector holding the output values for each sample.\n",
    "\n",
    "Below I plot the data, after defining a simple plotting function to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110d3b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(X, y):\n",
    "    plt.scatter(X, y, marker='x', color='red')\n",
    "\n",
    "plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by implementing the hypothesis and cost functions for linear regression.  The hypothesis function $h_\\theta(x)$ is the [linear combination](https://en.wikipedia.org/wiki/Linear_combination) of feature vector $x$ and parameter vector $\\theta$, and is used to predict an output value $y$.  Note that it is equivalent to the equation for a line as commonly taught in algebra, $y = mx + b$.\n",
    "\n",
    "$$\n",
    "h_\\theta(\\textbf{x}) = \\boldsymbol{\\theta}_0 + \\boldsymbol{\\theta}_1\\textbf{x}_1\n",
    "$$\n",
    "\n",
    "Below, the implementation for $h_\\theta(x)$ is *vectorized*, meaning it works with all input examples, instead of a single example $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(X, theta):\n",
    "    return np.matmul(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [cost function](https://en.wikipedia.org/wiki/Loss_function) used here is [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), or MSE.  This function $J(\\theta)$ represents the mean of the total difference between the actual $y$ values for all examples and what $h_\\theta(x)$ predicted they would be given parameters $\\theta$.  Squaring the difference ensures all differences are positive, and penalizes large errors proportionally more than small ones.\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}\\sum\\limits_{i = 1}^{m}(h_\\theta(\\textbf{X}_{i}) - \\textbf{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, theta):\n",
    "    return np.sum(np.square(predict(X, theta) - y)) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the cost with an initial guess for $\\theta$, a column of 1s is prepended onto the input data.  This allows us to vectorize the cost function, as well as make it usable for multiple linear regression later.  This first value $\\theta_0$ now behaves as a constant in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: [ 0.  0.]\n",
      "cost: 274.614970831\n"
     ]
    }
   ],
   "source": [
    "theta = np.zeros(2)\n",
    "X = np.column_stack((np.ones(len(X)), X))\n",
    "#y = rentals\n",
    "#cost = cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost:', cost(X, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now minimize the cost using the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm.  Intuitively, gradient descent takes small, linear hops down the slope of a function in each feature dimension, with the size of each hop determined by the partial derivative of the cost function with respect to that feature and a learning rate multiplier $\\alpha$.  If tuned properly, the algorithm converges on a global minimum by iteratively adjusting feature weights $\\theta$ of the cost function, as shown here for two feature dimensions:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0} J(\\theta_0,\\theta_1) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1} J(\\theta_0,\\theta_1) \n",
    "\\end{align}\n",
    "\n",
    "The update rule each iteration then becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_0 & := \\theta_0 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)}) \\\\\n",
    "\\theta_1 & := \\theta_1 - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_1^{(i)} \\\\\n",
    "\\end{align}\n",
    "\n",
    "See [here](http://mccormickml.com/2014/03/04/gradient-descent-derivation/) for a more detailed explanation of how the update equations are derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_cost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7892f4ce0076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"theta:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_cost' is not defined"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    theta = np.zeros(2)\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        t0 = theta[0] - (alpha / m) * np.sum(np.dot(X, theta) - y)\n",
    "        t1 = theta[1] - (alpha / m) * np.sum((np.dot(X, theta) - y) * X[:,1])\n",
    "        theta = np.array([t0, t1])\n",
    "\n",
    "    return theta\n",
    "\n",
    "iterations = 5000\n",
    "alpha = 0.1\n",
    "\n",
    "theta = gradient_descent(X, y, alpha, iterations)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print(\"theta:\", theta)\n",
    "print('cost:', compute_cost(X, y, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the values of $\\theta$ chosen by the algorithm using a few different visualizations, first by plotting $h_\\theta(x)$ against the input data.  The results show the expected correlation between temperature and rentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(temps, rentals, marker='x', color='red')\n",
    "plt.xlabel('Normalized Temperature in C')\n",
    "plt.ylabel('Bike Rentals in 1000s')\n",
    "samples = np.linspace(min(temps), max(temps))\n",
    "plt.plot(samples, theta[0] + theta[1] * samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surface plot is a better illustration of how gradient descent approaches a global minimum, plotting the values for $\\theta$ against their associated cost.  This requires a bit more code than an implementation in Octave / MATLAB, largely due to how the input data is generated and fed to the surface plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "Xs, Ys = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-40, 40, 50))\n",
    "Zs = np.array([compute_cost(X, y, [t0, t1]) for t0, t1 in zip(np.ravel(Xs), np.ravel(Ys))])\n",
    "Zs = np.reshape(Zs, Xs.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "ax.set_xlabel(r't0')\n",
    "ax.set_ylabel(r't1')\n",
    "ax.set_zlabel(r'cost')\n",
    "ax.view_init(elev=25, azim=40)\n",
    "ax.plot_surface(Xs, Ys, Zs, cmap=cm.rainbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a countour plot reveals slices of that surface plot in 2D space, and can show the resulting $\\theta$ values sitting exactly at the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "ax.plot(theta[0], theta[1], 'r*')\n",
    "plt.contour(Xs, Ys, Zs, np.logspace(-3, 3, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we reload the data and add two more features, humidity and windspeed.\n",
    "\n",
    "Before implementing gradient descent for multiple variables, we'll also apply [feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) to normalize feature values, preventing any one of them from disproportionately influencing the results, as well as helping gradient descent converge more quickly.  In this case, each feature value is adjusted by subtracting the mean and dividing the result by the standard deviation of all values for that feature:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "More details on feature scaling and normalization can be found [here](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    n_features = X.shape[1]\n",
    "    means = np.array([np.mean(X[:,i]) for i in range(n_features)])\n",
    "    stddevs = np.array([np.std(X[:,i]) for i in range(n_features)])\n",
    "    normalized = (X - means) / stddevs\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "X = data.as_matrix(columns=['atemp', 'hum', 'windspeed'])\n",
    "X = feature_normalize(X)\n",
    "X = np.column_stack((np.ones(len(X)), X))\n",
    "\n",
    "y = data['cnt'].values / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement gradient descent for any number of features.  Fortunately, the update step generalizes easily, and can be vectorized to avoid iterating through $\\theta_j$ values as might be suggested by the single variable implementation above:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi(X, y, theta, alpha, iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    m = len(X)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        gradient = (1/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta = gradient_descent_multi(X, y, theta, alpha, iterations)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it's now more difficult to evaluate the results visually, but we can check them a totally different method of calculating the answer, the [normal equation](http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/).  This solves directly for the solution without iteration specifying an $\\alpha$ value, although it begins to perform worse than gradient descent with large (10,000+) numbers of features.\n",
    "\n",
    "$$\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def normal_eq(X, y):\n",
    "    return inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "theta = normal_eq(X, y)\n",
    "cost = compute_cost(X, y, theta)\n",
    "\n",
    "print('theta:', theta)\n",
    "print('cost:', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\theta$ values and costs for each implementation are identical, so we can have a high degree of confidence they are correct.\n",
    "\n",
    "## Linear Regression in Tensorflow\n",
    "\n",
    "Tensorflow offers significantly higher-level abstractions to work with, representing the algorithm as a computational graph.  It has a built-in gradient descent optimizer that can minimize the cost function without us having to define the gradient manually.\n",
    "\n",
    "We'll begin by reloading the data and adapting it to more Tensorflow-friendly data structures and terminology.  Features are still normalized as before, but the added column of 1s is absent: the constant is treated separately as a *bias* variable, the previous $\\theta$ values are now *weights*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = data.as_matrix(columns=['atemp', 'hum', 'windspeed'])\n",
    "X = feature_normalize(X)\n",
    "y = data['cnt'].values / 1000\n",
    "y = y.reshape((-1, 1))\n",
    "\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "examples = tf.placeholder(tf.float32, [m,n])\n",
    "labels = tf.placeholder(tf.float32, [m,1])\n",
    "weights = tf.Variable(tf.zeros([n,1], dtype=np.float32), name='weight')\n",
    "bias = tf.Variable(tf.zeros([1], dtype=np.float32), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire gradient descent occurs below in only three lines of code.  All that's needed is to define the hypothesis and cost functions, and then a gradient descent optimizer to find the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis = tf.add(tf.matmul(examples, weights), bias)\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - y)) / (2 * m)\n",
    "optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is now ready to use, and all the remains is to start up a session, run the optimizer iteratively, and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(1, iterations):\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            examples: X,\n",
    "            labels: y\n",
    "        }) \n",
    "        \n",
    "    print('bias:', sess.run(bias))\n",
    "    print('weights:', sess.run(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias and weight values are identical to the $\\theta$ values calculated in both implementations previously, so the Tensorflow implementation of the algorithm looks correct.\n",
    "\n",
    "You can find the IPython notebook for this post on [GitHub](https://github.com/crsmithdev/notebooks/blob/master/ml-linear-regression/ml-linear-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:notebooks]",
   "language": "python",
   "name": "conda-env-notebooks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
